\section{Introduction}

Game theory is a mathematical framework for studying strategic interaction between self-interested agents, called players. In an $n$-player normal-form game, each player $i = 1, \cdots, n,$ selects a strategy $x^i \in \XX^i$  and receives a reward $R^i ( x^1, \dots, x^n )$, which depends on the collective \textit{strategy profile} $\bx = ( x^1, \dots, x^n) =: (x^i , \bx^{-i})$. Player $i$'s optimization problem is to \emph{best respond} to the strategy $\bx^{-i}$ of its counterparts, choosing $x^i \in \XX^i$ to maximize $R^i ( x^i, \bx^{-i})$. Game theoretic models are pervasive in machine learning, appearing in fields such as multi-agent systems \cite{gronauer2022multi}, multi-objective reinforcement learning \cite{hayes2022practical}, and adversarial model training \cite{bose2020adversarial},  among many others.  

In multi-agent reinforcement learning (MARL), players use learning algorithms to revise their strategies in response to the observed history of play, producing a sequence $\{ \widehat{\bx}_t \}_{t \geq 1}$ in the set of strategy profiles $\bX := \XX^1 \times \cdots \times \XX^n$. Due to the coupled reward structure of multi-agent systems, each player's learning problem involves a moving target: since an individual's reward function depends on the strategies of the others, strategy revision by one agent prompts other agents to revise their own strategies. Convergence analysis of MARL algorithms can therefore be difficult, and the development of tools for such analysis is an important aspect of multi-agent learning theory. 

A strategy profile $( x_{*}^{i} )_{i=1}^n$ is called a \textit{Nash equilibrium} if all players simultaneously best respond to one another. Nash equilibrium is a concept of central importance in game theory, and the tasks of computing, approximating, and learning Nash equilibrium have attracted enduring attention in theoretical machine learning \cite{singh2000nash,jafari2001no,daskalakis2010learning,nowe2012game,flokas2020no,hsieh2021adaptive,lu2023two}. Convergence to equilibrium strategies has long been a predominant, but not unique, design goal in MARL \cite{zhang2021multi}. In this paper, we study mathematical structure of normal-form games with the twin objectives of \textit{(i)} better understanding the capabilities or limitations of existing MARL algorithms and \textit{(ii)} producing insights for the design of new MARL algorithms.


A number of MARL algorithms approximate dynamical systems $\{ \bx_t \}_{t \geq 1}$ on the set of strategy profiles $\bX$ in which the next strategy for player $i$ is selected as $x^i_{t+1} = f^i ( \bx_t)$, where $\bx_t = ( x^1_t, \dots, x^n_t )$ is the strategy profile in period $t$. A sampling of such algorithms will be offered shortly. This approach facilitates analysis of the algorithm, as one separately considers the convergence of $\{ \bx_t \}_{t \geq 1}$ induced by the update functions $\{ f^i \}_{i = 1}^n$, on one hand, and the approximation of $\{ \bx_t \}_{t \geq 1}$ by the algorithm's iterates $\{ \widehat{\bx}_t \}_{t \geq 1}$ on the other. In this work, we consider update functions that satisfy a quasi-rationality condition called \emph{satisficing:} when an agent is best responding, the update rule instructs the agent to continue using this strategy. That is, if $x^i $ is a best response to $\bx^{-i}$, then $f^i ( x^i, \bx^{-i}) = x^i.$ This quasi-rationality constraint generalizes the best response update and is desirable for stability of the resulting dynamics, as it guarantees that Nash equilibria are invariant under the dynamics. Moreover, the satisficing condition is only quasi-rational, in that it imposes no constraint on strategy updates when an agent is not best responding, and so allows for exploratory strategy updates. Update rules that incorporate exploratory random search when a strategy is deemed unsatisfactory are common in MARL theory \cite{blume1993statistical,marden2012revisiting,chasparis2013aspiration, marden2014achieving}.


Our goal is to better understand the capabilities/limitations of MARL algorithms that use the satisficing principle to select successive strategies, potentially augmented with random exploration when an agent is not best responding. {Examples include \cite{foster2006regret, germano2007global,marden2009payoff,chien2011convergence,candogan2013near, arslan2017decentralized} and \cite{yongacoglu2023satisficing}.} Instead of studying a particular collection of strategy update functions, we abstract the problem to the level of sequences in $\bX$, which allows us to implicitly account for experimental strategy updates. A sequence $(\bx_t )_{t \geq 1}$ of strategy profiles is called a \emph{satisficing path} if, for each player $i$ and time $t$, one has that $x^i_{t+1} = x^i_t$ whenever $x^i_t$ is a best response to $\bx^{-i}_t$. The central research question of this paper is such:


\begin{center}
    \textit{For a normal-form game $\Gamma$ and an initial strategy profile $\bx_1$, is it always possible to construct a satisficing path from $\bx_1$ to a Nash equilibrium of the game $\Gamma$?}
\end{center}




Since many MARL algorithms operate using the satisficing principle (or otherwise approximate processes that involve satisficing update rules, e.g. \cite{swenson2018distributed}), the resolution of this question has implications for the effectiveness of such MARL algorithms. Indeed, the question has been answered in the affirmative for two-player normal-form games by \cite{foster2006regret} and for $n$-player symmetric Markov games by \cite{yongacoglu2023satisficing}, and in both classes of games this has directly lead to MARL algorithms with convergence guarantees for approximating equilibria. In addition to removing a theoretical obstacle, positive resolution of this question would establish that \textit{uncoordinated, distributed} random search can effectively assist Nash-seeking algorithms to achieve last-iterate convergence guarantees in a more general class of games than previously possible.

\noindent \textbf{Contributions.} We give a positive answer to the question above: for any finite $n$-player game $\Gamma$ and any initial strategy profile $\bx_1$, there exists a satisficing path beginning at $\bx_1$ and ending at a Nash equilibrium of $\Gamma$. This partially answers an open question posed by \cite{yongacoglu2023satisficing}. We prove this result by analytically constructing a satisficing path from an arbitrary initial strategy profile to a Nash equilibrium. Our approach is somewhat counterintuitive, in that it does not attempt to seek Nash equilibrium by improving the performance of unsatisfied players (players who are not best responding at a given strategy profile), but by updating strategies in  a way that \emph{increases} the number of unsatisfied players at each round. This tactic leverages the freedom afforded to unsatisfied players to explore their strategy space and avoids the challenge of cyclical strategy revision that occurs when agents attempt to best respond to their counterparts \cite{mertikopoulos2018cycles}. This insight provides a new approach to MARL algorithm design beyond the well-structured settings considered in prior work.

\noindent \textbf{Notation.} % 
We let $\simplex_A$ denote the set of probability measures over a set $A$. %
For $n\in \nn$, we let $[n] := \{ 1, 2, \dots, n \}$. For a point $x$, the Dirac measure centered at $x$ is denoted $\delta_x$. %
When discussing a fixed agent $i$, the remaining collection of agents are called $i$'s counterparts or counterplayers.