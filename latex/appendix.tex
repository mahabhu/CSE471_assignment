%\newpage 

\section*{Appendix: Proofs of technical lemmas}  \label{appendix}

We now discuss the properties of the auxiliary functions $\{ F^i : i \in[n]\}$ that were used in the proof of \Cref{theorem:main}, and we prove Lemma~\ref{lemma:equilibrium-on-boundary}. 

We remark that for each player $i \in [n]$, we identify their set of mixed strategies $\XX^i = \Delta_{\aa^i}$ with the probability simplex in $\rr^{\aa^i}$. Thus, $\XX^i$ inherits the Euclidean metric from $\rr^{| \aa^i |}$. Neighbourhoods and limits in $\XX^i$ (or its subsets) are defined with respect to this metric. Similarly, we inherit a Euclidean metric for $\bX$. For $\zeta > 0$, we let $N_{\zeta} ( \bx )$ denote the $\zeta$-neighbourhood of the strategy profile $\bx \in \bX$. 


 
\section{Properties of the auxiliary functions} \label{appendix:F-functions}

 We begin by discussing the properties of the auxiliary functions $\{ F^i : i \in [n] \}$, as they are relevant to characterizing best responses. The facts below are well known. For a reference, see the text of \cite{maschler2020game}.
 
Recall that for each player $i\in[n]$, the function $F^i : \bX \to \rr$ is defined as
\[
F^i ( x^i, \bx^{-i} ) = \max_{a^i \in \aa^i} R^i ( \delta_{a^i} , \bx^{-i} ) - R^i ( x^i, \bx^{-i} ), \quad \forall \bx \in \bX .
\]

We now show that for any $i \in [n]$, the following hold: 
\begin{enumerate}[label=\alph*.]
    \item $F^i$ is continuous on $\bX$,
    \item $F^i (\bx) \geq 0$ for all $\bx \in \bX$, and
    \item For any $\bx^{-i} \in \bX^{-i}$, a strategy $x^i$ is a best response to $\bx^{-i}$ if and only if $F^i (x^i, \bx^{-i}) = 0$.
\end{enumerate}

The expected reward function $R^i( \bx) = \ee_{\ba \sim \bx } \left[ r^i ( \ba) \right]$ can be expressed as a sum of products:
\[
% R^i( \bx) = \sum_{ \tilde{\ba} \in \bA } r^i ( \ba ) \pp_{\ba \sim \bx } \left( \ba = \tilde{\ba}  \right)
% = \sum_{\tilde{a}^1 \in \aa^1 } \cdots \sum_{\tilde{a}^n \in \aa^n} r^i ( \tilde{a}^1, \dots, \tilde{a}^n ) \prod_{j =1}^n x^j ( \tilde{a}^j ), \quad \forall \bx \in \bX .
R^i( \bx) = \sum_{ \tilde{\ba} \in \bA } r^i ( \ba ) \pp_{\ba \sim \bx } \left( \ba = \tilde{\ba}  \right)
= \sum_{\tilde{\ba} \in \bA} r^i ( \tilde{a}^1, \dots, \tilde{a}^n ) \prod_{j =1}^n x^j ( \tilde{a}^j ), \quad \forall \bx \in \bX .
\]

From this form, it is immediate that $R^i$ is continuous on $\bX$. Moreover, it can easily be shown that $R^i$ is multi-linear in $\bx$. That is, for any $j \in [n]$, fixing $\bx^{-j}$, we have that $x^j \mapsto R^i ( x^j, \bx^{-j})$ is linear.\footnote{Of course, scaling inputs of $R^i$ means the resulting argument is no longer a probability vector. However, one can simply linearly extend $R^i$ to be a function on $\rr^{d}$, where $d = \sum_{j=1}^n | \aa^j |$.}

Since $R^i$ is continuous on $\bX$ and $\aa^i$ is a finite set, one has that the pointwise maximum of finitely many continuous functions is continuous. Thus, the function
\[
\bx^{-i} \mapsto \max_{ a^i \in \aa^i } R^i \left( \delta_{a^i} , \bx^{-i} \right) 
\]
is continuous on $\bX^{-i}$. Since $F^i ( x^i, \bx^{-i} ) =  \max_{ a^i \in \aa^i } R^i \left( \delta_{a^i} , \bx^{-i} \right) - R^i ( x^i, \bx^{-i})$ is the difference of continuous functions, $F^i$ is also continuous. This proves item a.

From the multi-linearity of $R^i$, we have that, for fixed $\bx^{-i} \in \bX^{-i}$, the optimization problem $\sup_{ x^i \in \XX^i } R^i ( x^i, \bx^{-i} )$ is equivalent to a linear program
\[
\sup_{ x^i \in \rr^{\aa^i}} w_{\bx^{-i}}^{\top} x^i, \quad \text{ subject to }
    \begin{cases}
     1^{\top} x^i = 1 , \\
     x^i \geq 0
    \end{cases},
\]
where $w_{\bx^{-i}} \in \rr^{\aa^i}$ is a vector defined by $ w_{\bx^{-i}}(a^i) :=  R^i (\delta_{a^i}, \bx^{-i} ). $
% \[
% w_{\bx^{-i}}(a^i) := \sum_{\ba^{-i} \in \bA^{-i}} r^i ( a^i, \ba^{-i}) \prod_{j\not=i} x^j ( a^j ) = R^i (\delta_{a^i}, \bx^{-i} ). 
% \]

The vertices of the feasible set for the latter linear program are precisely the points $\{ \delta_{a^i} : a^i \in \aa^i \}$. This implies that $\max_{a^i} R^i (\delta_{a^i} , \bx^{-i} ) \geq R^i (x^i, \bx^{-i} ) $ for any $x^i, \bx^{-i}$. Items b and c follow. From this formulation, one can also see that a player $i \in [n]$ is satisfied at $\bx \in \bX$ if and only if its strategy $x^i$ is supported on the set of maximizers ${\rm argmax}_{a^i \in \aa^i} \{ R^i ( \delta_{a^i}, \bx^{-i} ) \}$. 

\ 

\section{Proof of Lemma~\ref{lemma:equilibrium-on-boundary}} \label{appendix:lemma}

Recall that in the proof of \Cref{theorem:main}, $\bx_{\star}$ was defined to be some strategy accessible from $\bx_k \in \bX$ such that all players unsatisfied at $\bx_k$ were satisfied at $\bx_{\star}$. The statement of Lemma~\ref{lemma:equilibrium-on-boundary} was the following.
\begin{itemize}
    \item[] \textbf{Lemma~\ref{lemma:equilibrium-on-boundary}} \textit{If $\Worse( \bx_k ) = \varnothing$, then there exists a sequence $\{ \by \}_{t = 1}^{\infty}$, with $\by_t \in \NoB(\bx_k)$ for each $t$, such that $     \lim_{t \to \infty} \by_t = \bx_{\star}. $ }
\end{itemize}


\begin{proof}
Suppose, to the contrary, that no such sequence exists. Then, there exists some $\zeta > 0$ such that for every $\bz \in \Acc(\bx_k)  \cap N_{\zeta} ( \bx_{\star} )$, one has $\bz \notin \NoB( \bx_k )$. That is, some player unsatisfied at $\bx_k$ is satisfied at $\bz$. Equivalently, for some $i \in \UnSat(\bx_k)$, we have $z^i \in \BR^i_0 ( \bz^{-i})$. This implies that for that player $i$, that value of $\zeta$, and the strategy profile $(z^i, \bz^{-i}) \in N_{\zeta}(\bx_{\star})$, $z^i$ is supported on the set $\argmax_{a^i \in \aa^i} \{ R^i ( \delta_{a^i} , \bz^{-i} ) \}$.

For each $\xi \geq 0$, we define a strategy profile $\bw_{\xi} \in \bX$ as follows:
\[
w^i_{\xi} := 
    \begin{cases} 
        (1 - \xi) x^i_k + \xi \Uniform ( \aa^i ),   &\text{ if } i \in \UnSat(\bx_k) \\
        x^i_k,                                      &\text{ else.}
    \end{cases}
\]

{
Note that we have defined $w^i_{\xi} = x^i_k$ for $i \in \Sat(\bx_k)$, which is to say that we change only the strategies of the unsatisfied players, meaning $\bw_{\xi} \in \Acc( \bx_k )$. We will show that if $\xi > 0$ is sufficiently small, then continuity of the functions $\{ F^i \}_{i \in [n] }$ guarantees that $\bw_{\xi} \in \NoB(\bx_k)$.

Indeed, player $i \in [n]$ is unsatisfied at $\bx_k$ if and only if it fails to best respond, $x^i_k \notin \BR^i_0 ( \bx^{-i}_k )$.  Using the function $F^i$, this is equivalent to $F^i ( x^i_k, \bx^{-i}_k )  > 0 $. For each player $i \in \UnSat( \bx_k )$, let $\sigma_i > 0 $ be such that $F^i ( \bx_k ) \geq \sigma_i > 0 $. Define $\bar{\sigma} = \min\{ \sigma_i : i \in \UnSat(\bx_k ) \}$. 

The following statement holds by the continuity of the functions $\{ F^i \}_{i = 1}^n$: for each player $i \in [n]$, there exists $e_i > 0 $ such that if a strategy profile $\bx$ belongs to the $e_i$ neighbourhood of $\bx_k$ (i.e. $\bx \in N_{e_i} ( \bx )$), then $| F^i ( \bx ) - F^i ( \bx_k ) | < \bar{\sigma} / 2$. Since $F^i ( \bx_k ) \geq \bar{\sigma}$, it follows that $F^i ( \bx ) > \bar{\sigma}/2 > 0$, and player $i$ is not best responding at $\bx \in N_{e_i} ( \bx ) $.

Let $\bar{e} := \min\{ e_i : i \in [n] \} $. By taking $\xi < \bar{e}/(2n)$, one has that $\bw_{\xi} \in N_{ \bar{e}  } ( \bx_k ) $. From the preceding remarks, one can see that $\UnSat( \bx_k ) \subseteq \UnSat(\bw_{\xi})$, since all players who were unsatisfied at $\bx_k$ remain unsatisfied at $\bw_{\xi}$. Since $w^j_{\xi} = x^j_k$ for any player $j \in \Sat( \bx_k )$, one also has that $\bw_{\xi} \in \Acc ( \bx_k )$. These two parts combine to show that $\bw_{\xi} \in \NoBetter ( \bx_ k )$.
}

Fixing $\xi > 0$ at a sufficiently small value ($\xi \in (0, \bar{e}/2n)$), the preceding deductions show that $\bw_{\xi} \in \NoB(\bx_k)$. By the earlier discussion, we have that $\bw_{\xi} \notin N_{\zeta} ( \bx_{\star})$. 

A very important aspect of this construction is that $w^i_{\xi} ( a^i) > 0$ for each $i \in \UnSat(\bx_k)$ and action $a^i \in \aa^i$, so that $w^i_{\xi}$ is fully mixed for each player who was unsatisfied at $\bx_k$.

Next, for each $\lambda \in [0,1]$ and player $i \in \UnSat(\bx_k)$, we define
\[
z^i_{\lambda} = (1-\lambda) x^i_{\star} + \lambda w^i_{\xi} .
\]
We also define $z^i_{\lambda} = x^i_k$ for players $i \in \Sat(\bx_k)$. For sufficiently small values of $\lambda$, say $\lambda \leq \bar{\lambda}$, we have that $\bz_{\lambda} \in N_{\zeta}(\bx_{\star})$, which implies $\bz_{\lambda} \notin \NoB(\bx_k)$. 

This implies that there exists a player $i^{\dagger} \in \UnSat(\bx_k)$ for whom
\[
z^{i^{\dagger}}_{\lambda} \in \BR^{i^{\dagger}}_0 \left( \bz^{-i^{\dagger}}_\lambda \right), \text{ for infinitely many } \lambda \in \big( 0, \bar{\lambda} \big] . 
\]

(The existence of such a player is perhaps not obvious. As we previously noted, for $\lambda < \bar{\lambda}$, we have $\bz_{\lambda} \notin \NoB(\bx_k)$, which means there exists \emph{some} player $i^{\dagger}(\lambda)$ that was unsatisfied at $\bx_k$ and is satisfied at $\bz_{\lambda}$. The identity of this player may change with $\lambda$. To see that some particular individual must satisfy this best response condition infinitely often, one can apply the pigeonhole principle to the set $\{ \bar{\lambda}, \bar{\lambda}/2, \dots, \bar{\lambda}/m \}$ for arbitrarily large $m$.)

By our definition of $z^{\idagger}_{\lambda}$ as a convex combination involving $\Uniform(\aa^{\idagger})$, we have that $z^{\idagger}_{\lambda}$ is fully mixed and puts positive probability on each action in $\aa^{\idagger}$. Using the characterization involving $F^{\idagger}$, the fact that $z^{i^{\dagger}}_{\lambda} \in \BR^{i^{\dagger}}_0 \left( \bz^{-i^{\dagger}}_\lambda \right)$ and the fact that $z^{\idagger}_\lambda$ is fully mixed together imply that $R^{\idagger} ( \delta_a , \bz^{-\idagger}_{\lambda} ) =  R^{\idagger} ( \delta_{a'} , \bz^{-\idagger}_{\lambda} ),$ for any $a, a' \in \aa^{\idagger}$. This can be equivalently re-written as
%
%
% \begin{align}
% \sum_{\ba^{-\idagger}} r^{\idagger} (a, \ba^{-\idagger}) \prod_{j \not= \idagger} \left\{ (1-\lambda) x^j_{\star} ( a^j ) + \lambda w^j_{\xi} ( a^j ) \right\} =  \sum_{\ba^{-\idagger}} r^{\idagger} (a', \ba^{-\idagger}) \prod_{j \not= \idagger} \left\{ (1-\lambda) x^j_{\star} ( a^j ) + \lambda w^j_{\xi} ( a^j ) \right\} \nonumber \\
% %
% \iff &\sum_{\ba^{-\idagger}} \left[ r^{\idagger} (a, \ba^{-\idagger}) - r^{\idagger} (a', \ba^{-\idagger}) \right] \prod_{j \not= \idagger} \left\{ (1-\lambda) x^j_{\star} ( a^j ) + \lambda w^j_{\xi} ( a^j ) \right\} = 0
% %
% \label{eq:indifference}
% \end{align}
\begin{align}
  &\sum_{\ba^{-\idagger}} r^{\idagger} (a, \ba^{-\idagger}) \prod_{j \not= \idagger} \left\{ (1-\lambda) x^j_{\star} ( a^j ) + \lambda w^j_{\xi} ( a^j ) \right\} \nonumber \\
= &\sum_{\ba^{-\idagger}} r^{\idagger} (a', \ba^{-\idagger}) \prod_{j \not= \idagger} \left\{ (1-\lambda) x^j_{\star} ( a^j ) + \lambda w^j_{\xi} ( a^j ) \right\} \nonumber \\
%
\iff &\sum_{\ba^{-\idagger}} \left[ r^{\idagger} (a, \ba^{-\idagger}) - r^{\idagger} (a', \ba^{-\idagger}) \right] \prod_{j \not= \idagger} \left\{ (1-\lambda) x^j_{\star} ( a^j ) + \lambda w^j_{\xi} ( a^j ) \right\} = 0
%
\label{eq:indifference}
\end{align}
for any $a, a' \in \aa^{\idagger}$. 


The lefthand side of the final equality \eqref{eq:indifference} is a polynomial in $\lambda$ of finite degree, but admits infinitely many solutions (from our choice of $\idagger$). This implies that it is the zero polynomial. In turn, this implies that the left side of \eqref{eq:indifference} holds for any $\lambda \in [0,1]$, and in particular for $\lambda = 1$. This means $z^{\idagger}_1 \in \BR^{\idagger}_0 ( \bz^{-\idagger}_1 )$, meaning $\bz_1 \notin \NoB(\bx_k)$. On the other hand, we have  $\bz_1 = \bw_{\xi} \in \NoB(\bx_k)$, a contradiction. 

Thus, we see that there exists a sequence $\{ \by_t \}_{t = 1}^{\infty}$, with $\by_t \in \NoB(\bx_k)$ for all $t$, such that $\lim_{t \to \infty} \by_t = \bx_{\star}$.
%
\end{proof} 



\section{Markov games: model and connections to Theorem 1}  \label{appendix:markov-games}

Markov games are popular model in the field of multi-agent reinforcement learning. Since the model is quite standard, we offer a short description of the fundamental objects and notations, and we then describe connections between \Cref{theorem:main} and a possible extension to multi-state Markov games. 

%(Footnote to appendix.) %\footnote{\brown{This paper considers policies that are time invariant. These so-called stationary (Markov) policies are the most common class of policies studied in MARL. In principle, one could study a broader class of policies, in which action selections can depend on the observed history of the system. However, if one's counterplayers use stationary policies, restricting one's attention to stationary policies entails no loss of optimality. \red{cite Levy's stochastic games paper}.}} 





A Markov game with $n$ players and discounted rewards is described by a list $\bG = (n, \SS, \bA, \TT , \br, \gamma )$, where $\SS$ is a finite set of statess, $\bA = \aa^1 \times \cdots \times \aa^n$ is a finite set of action profiles, and $\br = ( r^i )_{i=1}^n$ is a collection of reward functions, where $r^i : \SS \times \bA \to \rr$ describes the reward to player $i$. A transition probability function $\TT \in \PP ( \SS | \SS \times \bA )$ governs the evolution of the state process, described below, and a discount factor $\gamma \in (0,1)$ is used to aggregate rewards across time. 


\noindent \textbf{Description of play.} %\footnote{\red{Markov games are popular model in the field of multi-agent reinforcement learning (MARL). Since the model is quite standard, we offer an abbreviated description here, and refer the reader to (Cite Levy 2015, Littman 1994) and Appendix ? for a longer treatment.}} 
Markov games are played across discrete time, indexed by $t \in \nn$. At time $t$, the state variable is denoted $s_t \in \SS$ and each player $i \in [n]$ selects an action $a^i_t \in \aa^i$ according to a distribution $\pi^i ( \cdot | s_t)$: $a^i_t \sim \pi^i ( \cdot | s_t )$. The transition probability function $\pi^i \in \PP ( \aa^i | \SS )$ is called player $i$'s \emph{policy}, and we denote player $i$'s set of policies by $\Pi^i := \PP ( \aa^i | \SS )$. %(Footnote to appendix.) %\footnote{\brown{This paper considers policies that are time invariant. These so-called stationary (Markov) policies are the most common class of policies studied in MARL. In principle, one could study a broader class of policies, in which action selections can depend on the observed history of the system. However, if one's counterplayers use stationary policies, restricting one's attention to stationary policies entails no loss of optimality. \red{cite Levy's stochastic games paper}.}} 
For any time $t \in \nn$, the collection of actions $\{ a^i_t \}_{i=1}^n$ is mutually conditionally independent given $s_t$. Upon selection of the action profile $\ba_t := (a^i_t )_{i=1}^n$, each player $i$ receives the reward $r^i ( s_t, \ba_t )$, and the state transitions from $s_t$ to $s_{t+1}$ according to $s_{t+1} \sim \TT ( \cdot | s_t, \ba_t)$.



Player $i$'s performance criterion is its expected $\gamma$-discounted return, which depends on the state variable and the collective policy profile $\bpi := ( \pi^1, \dots, \pi^n )$, which we also denote by $(\pi^i, \bpi^{-i})$ to isolate player $i$'s policy. We let $\bPi := \Pi^1 \times \cdots \times \Pi^n$ denote the set of policy profiles. For each pair $(\bpi, s) \in \bPi \times \SS$,   player $i$'s expected $\gamma$-discounted return is given by 
\[
V^i ( \pi^i, \bpi^{-i}, s ) := \ee_{\bpi } \left[ \sum_{t =1}^\infty \gamma^{t-1} r^i (s_t, \ba_t ) \middle| s_1 = s      \right],
\]
where $\ee_{\bpi}$ denotes that for every $t \geq 1$, we have that $a^j_t \sim \pi^j(\cdot | s_t )$ for each player $j \in [n]$ and, implicitly, $s_{t+1} \sim \TT ( \cdot | s_t,  \ba_t )$. 



\begin{definition}
    For $\epsilon \geq 0$, a policy $\pi^i_{*} \in \Pi^i$ is called an $\epsilon$-best response to $\bpi^{-i}$ if
        \[
        V^i ( \pi^i_{*}, \bpi^{-i} , s ) \geq V^i ( \pi^i, \bpi^{-i}, s ) - \epsilon  , \quad \forall  \pi^i \in \Pi^i, \quad \forall s \in \SS . 
        \]
\end{definition}


\begin{definition}
    For $\epsilon \geq 0$, a policy profile $\bpi_{*} = ( \pi^i_{*}, \bpi^{-i}_{*} ) \in \bPi^i$ is called a \emph{Markov perfect $\epsilon$-equilibrium} if, for each player $i \in [n]$, $\pi^i_{*}$ is an $\epsilon$-best response to $\bpi^{-i}_{*}$. 
\end{definition}

Putting $\epsilon = 0$ into the definitions above, we recover the classical definitions of best responding and Markov perfect equilibrium. In analogy to normal-form games, we use $\BR^i_{\epsilon} ( \bpi^{-i} ) \subseteq \Pi^i$ to denote player $i$'s set of $\epsilon$-best-responses to a given counterplayer policy profile $\bpi^{-i}$. 


\subsection*{Remarks on Markov games}

As is conventional in the literature on MARL, we focus on policies that are stationary, Markovian, and possibly randomized. That is, we focus on policies for player $i$ that map states $s_t \in \SS$ to distributions over the agent's action set $\aa^i$  and sample each action $a^i_t$ from that distribution in a time-invariant and history-independent manner. In principle, agents could use policies that depend also on the time index $t$ or on the history of states and actions. However, the bulk of works on MARL consider this simpler class of policies and this is justifiable for several reasons. We refer the reader to \cite{levy2013discounted} for a summary of such justifications. 

Markov games generalize both normal-form games (taking the state space $\SS$ to be a singleton) and also MDPs (taking the number of players $n=1$). Moreover, when player $i$'s counterplayers follow a stationary policy $\bpi^{-i} \in \bPi^{-i}$, as assumed in this work, player $i$'s stochastic control problem is equivalent to a single-agent MDP (whose problem data depend on $\bpi^{-i}$). It follows that player $i$'s set of (stationary) best responses to $\bpi^{-i}$ is non-empty. Furthermore, player $i$'s best response condition can be characterized using the familiar action value (Q-) functions of reinforcement learning theory. We briefly summarize this below.

In addition to the objective criterion $V^i ( \pi^i, \bpi^{-i}, s )$, which is called the value function, we may also define the action value function $Q^i$ for player $i$ as
\[
Q^i ( \pi^i, \bpi^{-i} , s, a^i ) := \ee_{\bpi} \left[ \sum_{t=1}^{\infty} \gamma^{t-1} r^i( s_t, \ba_t ) \middle| s_1 = s, a^i_1 = a^i  \right], 
\]
for $(\pi^i, \bpi^{-i} ) \in \bPi, (s,a^i) \in \SS \times \aa^i . $ 

We further define an optimal action value function for player $i$ against $\bpi^{-i}$, denoted $Q^{*i}_{\bpi^{-i}}$, as 
\[
Q^{*i}_{\bpi^{-i}} ( s, a^i ) := \max_{ \pi^i_{*} \in \Pi^i } Q^i ( \pi^i_{*}, \bpi^{-i} , s , a^i ) , \quad \forall (s,a^i ) \in \SS \times \aa^i . 
\]

For any policy $\bpi = (\pi^i, \bpi^{-i})$, one can express player $i$'s value function using its Q-function and conditional expectations as $V^i ( \bpi, s ) = \sum_{a^i} \pi^i(a^i|s) Q^i ( \bpi, s,a^i )$. From this, it follows that 
\[
\max_{a^i \in \aa^i } Q^{*i}_{\bpi^{-i}} ( s, a^i ) = \max_{\pi^i_{*} \in \Pi^i } V^i ( \pi^i_{*} , \bpi^{-i}, s ) , \quad \forall s \in \SS.  
\]

This equality allows us to characterize best responses using a function $f^i : \bPi \to \rr$, analogous to the function $F^i$ appearing in the normal-form case. We define $f^i ( \bpi ) $ as 
\[
f^i ( \pi^i, \bpi^{-i} ) = \max_{ s \in \SS } \left[  \max_{ a^i_{\star} \in \aa^i } Q^{*i}_{\bpi^{-i}} ( s, a^i_{\star}) - V^i ( \bpi, s )          \right], \quad \forall \bpi \in \bPi. 
\]

The functions $\{ f^i \}_{i=1}^n$ defined above possess the three properties we required of the functions $\{ F^i \}_{i = 1}^n$ in the proof of \Cref{theorem:main}: (a) $f^i$ is continuous on $\bPi$ \cite{yongacoglu2023satisficing}, (b) $f^i ( \bpi ) \geq 0$ for all $\bpi \in \bpi$, and (c) $f^i ( \pi^i, \bpi^{-i} ) = 0 $ if and only if $\pi^i$ is a best response to $\bpi^{-i}$. 

\subsection*{On extending Theorem 1 to Markov games}

We now turn our attention to the task of extending \Cref{theorem:main} to Markov games. Following the proof of \Cref{theorem:main}, virtually all steps can be reproduced in the multi-state setting. To begin, one can construct a satisficing path $\bpi_1, \bpi_2, \dots, \bpi_k$ by growing the set of unsatisfied players at each iteration until either $\UnSat(\bpi_k) = [n]$ or $\Worse( \bpi_k ) = \varnothing$. In the latter case, one can consider the subgame involving only the players in $\UnSat(\bpi_k)$ and obtain a Markov perfect equilibrium $\tilde{\bpi}_{\star}$ for that subgame, which can then be extended to a policy profile $\bpi_{\star} \in \Acc( \bpi_k )$ by putting 
\[
\pi^i_{\star}=  
                \begin{cases}
                \tilde{\pi}^i_{\star}, &\text{ if } i \in \UnSat(\bpi_k), \\
                \pi^i_k, &\text{ if } i \in \Sat(\bpi_k). 
                \end{cases}
\]

To complete the extension of \Cref{theorem:main} to Markov games, one must show that this policy $\bpi_{\star} \in \bPi$ is a Markov perfect equilibrium of the $n$-player Markov game. Since the functions $\{ f^i \}_{i = 1}^n$ also satisfy the continuity and semi-definiteness properties described in \Cref{appendix:F-functions}, one possible technique for completing this proof involves showing that the policy $\bpi_{\star}$ is a limit point of the set $\NoBetter(\bpi_k)$. In other words, one possible technique for completing this proof requires extending \Cref{lemma:equilibrium-on-boundary} to the multi-state case. 

Up to this point, analysis of the stateless case and the multi-state case have been conducted perfectly in parallel. However, it is in the extension of \Cref{lemma:equilibrium-on-boundary} that the presence of a state leads to a discrepancy in the analysis that will necessitate a novel proof technique for the extension of \Cref{theorem:main} to Markov games. We elaborate below on this discrepancy. %As we will explain, the complication appears to be connected to our chosen proof technique in the normal-form case rather than to a fundamental obstruction. 

\paragraph{Normal-form game analysis.} In the context of finite normal-form games, our proof of \Cref{lemma:equilibrium-on-boundary} in \Cref{appendix:lemma} involves a proof by contradiction that exploits the explicit form of an indifference condition in the stateless case. In simple terms, if a player $i$ is best responding \emph{and} placing positive probability on every action, then any two actions offer equal expected payoff. In symbols, we note that $R^i ( \delta_{a^i_1} , \bx^{-i} ) = R^i ( \delta_{a^i_2} , \bx^{-i} )$ if and only if
\begin{align*}
&\sum_{ \ba^{-i} \in \bA^{-i} } \left[ r^i ( a^i_1, \ba^{-i} ) - r^i ( a^i_2, \ba^{-i} ) \right] \pp_{ \bx^{-i} } ( \ba^{-i } )  \\
= &\sum_{ \ba^{-i} \in \bA^{-i} } \left[ r^i ( a^i_1, \ba^{-i} ) - r^i ( a^i_2, \ba^{-i} ) \right] \prod_{ j \not= i} \left\{ x^j ( a^j ) \right\} = 0 . 
\end{align*}

For reasons that will be clarified below, we refer to the expressions $\left[ r^i ( a^i_1, \ba^{-i} ) - r^i ( a^i_2, \ba^{-i} ) \right]$ as \emph{coefficient terms}, and we refer to the terms $\pp_{ \bx^{-i} } ( \ba^{-i} ) = \prod_{ j \not= i} \left\{ x^j ( a^j ) \right\}  $ as strategy-dependent terms. We remark that in the case of normal-form games, the coefficient terms above do not depend on the strategy $\bx^{-i}$. 

Our proof of \Cref{lemma:equilibrium-on-boundary} in \Cref{appendix:lemma} considered a one-parameter family of strategies parameterized by $\lambda \in [ 0, 1]$. As part of an intricate proof by contradiction, we obtained an indifference condition, \eqref{eq:indifference}, for a player $\idagger$ who played each action with positive probability while also best responding. Due to the explicit parameterization by $\lambda$ of the strategy $\bz_{\lambda}$, we are able to recognize that the indifference condition in \eqref{eq:indifference} is characterized by the roots of a polynomial in $\lambda$. Critically, the lefthand-side of \eqref{eq:indifference} is a polynomial in $\lambda$ because the coefficient terms do not depend on the strategy $\bz_{\lambda}$ and hence do not depend on $\lambda$, while the strategy-dependent terms are polynomials in $\lambda$. %As we will see momentarily, it is at this point in our proof technique that the absence of a state was helpful, since the presence of a state will complicate matters when attempting to perform a parallel analysis for Markov games. 

\paragraph{Markov game analysis.} %We conclude this section with a demonstration that the normal-form analysis used to prove \Cref{lemma:equilibrium-on-boundary} in \Cref{appendix:lemma} cannot be used to prove an extension of \Cref{lemma:equilibrium-on-boundary} to the setting of Markov games. However, we also observe that this is a limitation of the proof technique and not indicative of a fundamental obstacle, and we conclude that an alternative analytic approach is needed. 
By contrast, we now study indifference conditions in Markov games. Consider an agent $i$ who is best responding to a policy $\bpi^{-i}$ and places positive probability on actions $a^i_1$ and $a^i_2$ in state $s$. The optimality condition is turned into an indifference condition between $a^i_1$ and $a^i_2$ in state $s$ as follows:
\[
Q^{*i}_{\bpi^{-i}} ( s, a^i_1 ) = Q^{*i}_{\bpi^{-i}} ( s, a^i_2 ) = \max_{a^i \in \aa^i } Q^{*i}_{\bpi^{-i}} ( s, a^i ) .
\]

One can show that $Q^{*i}_{ \bpi^{-i }}$ satisfies the following equality for any $(s,a^i) \in \SS \times \aa^i$:
\[
Q^{*i}_{\bpi^{-i}} ( s, a^i ) = \sum_{\ba^{-i} \in \bA^{-i}} \left[ r^i ( s, a^i, \ba^{-i} ) + \gamma \sum_{ s' \in \SS } \TT ( s' | s, a^i, \ba^{-i} ) \max_{ a^i_{\star} \in \aa^i } Q^{*i}_{\bpi^{-i}} ( s' , a^i_{\star} )    \right] \pp_{\bpi} ( \ba^{-i} | s ) ,
\]
where $\pp_{\bpi} ( \ba^{-i} | s ) = \prod_{j \not= i } \pi^j ( a^j | s )$ denotes the probability of the action profile $\ba^{-i}$ in state $s$ under policy $\bpi$. In analogy to the normal-form case, we refer to $ \pp_{\bpi} ( \ba^{-i} | s )$ as the strategy-dependent term and we refer to the term enclosed in square brackets as the coefficient term. However, unlike the normal-form case, here it is clear that the (so-called) coefficient term also depends on the policy $\bpi^{-i}$, through the term $\max_{a^i_{\star} \in \aa^i } Q^{*i}_{\bpi^{-i}} ( s, a^i_{\star})$. 

Suppose now that we obtain a one-parameter family of policies $\{ \bvarpi_{\lambda} : 0 \leq \lambda \leq 1 \}$ parameterized by some $\lambda \in [0,1]$, in analogy to our construction of $\bz_{\lambda}$ in \Cref{appendix:lemma}. Since the coefficient term depends on the policy of player $i$'s counterplayers, one has that the indifference condition 
\[ 
Q^{*i}_{\bvarpi^{-i}_{\lambda}} ( s, a^i_1 ) - Q^{*i}_{\bvarpi^{-i}_{\lambda}} ( s, a^i_2 ) = 0
\]
cannot generally be characterized by the roots of a polynomial in the parameter $\lambda$.\footnote{Although this indifference condition does not generally yield a polynomial in $\lambda$, one can easily find special cases of Markov games in which it does. For instance, if player $i$'s action does not influence transition probabilities, the indifference condition will yield a polynomial and the normal-form proof technique will go through without modification.}

Without characterization of the indifference condition as a polynomial in the policy parameter, our proof technique in \Cref{appendix:lemma} becomes unsuitable for the multi-state setting: we cannot invoke the fundamental theorem of algebra to conclude that the coefficient terms are identically zero, and thus we cannot obtain the contradiction critical to our proof by contradiction, where we found that player $\idagger$ is in fact indifferent even at the extreme parameter value of $\lambda = 1$. 

In summary, the proof technique employed in \Cref{appendix:lemma} to prove \Cref{lemma:equilibrium-on-boundary} relies crucially on the specific explicit form of the indifference condition in stateless, finite normal-form games. Passing to the multi-state setting, the analogous indifference condition takes a different form, and so the specific derivations cannot be repurposed for a simple extension of \Cref{lemma:equilibrium-on-boundary}. However, it is also important to recognize that this phenomenon is a limitation of the proof technique and does not pose a fundamental obstacle to the generalization of \Cref{theorem:main} \emph{per se}. Indeed, the remaining elements of the proof of \Cref{theorem:main} carry over seemlessly to the multi-state case, including various continuity conditions for functions characterizing best responses. It therefore seems promising that one can generalize \Cref{theorem:main} to apply to Markov games by applying similar machinery as used in this paper but substituting a different proof for that of \Cref{lemma:equilibrium-on-boundary} to take advantage of topological or geometric structure shared by both normal-form and Markov games. We leave this as an interesting open question for future research. 